fp16_addmm_fwd:
  op: addmm
  args: --op addmm --baseline aten_addmm --metrics tflops,speedup --only triton_addmm,aten_addmm
bf16xint16_gemm_fwd:
  op: bf16xint16_gemm
  args: --op bf16xint16_gemm --metrics tflops --only bf16xbf16
bf16_flash_attention_fwd:
  op: flash_attention
  args: --op flash_attention --baseline flash_v3 --metrics tflops,speedup --only triton_tutorial_flash_v2,triton_tutorial_flash_v2_opt,flash_v3
bf16_flex_attention_fwd:
  op: flex_attention
  args: --op flex_attention --baseline eager --metrics tflops,speedup --only compiled,eager
fp8_attention_fwd:
  op: fp8_attention
  args: --op fp8_attention --metrics tflops --only triton_flash_v2_tma
fp8_fused_quant_gemm_rowwise_fwd:
  op: fp8_fused_quant_gemm_rowwise
  args: --op fp8_fused_quant_gemm_rowwise --metrics tflops --only rms_norm_fused
fp8_gemm_fwd:
  op: fp8_gemm
  args: --op fp8_gemm --baseline torch_fp8_gemm --metrics tflops,speedup --only triton_tma_persistent_fp8_gemm,torch_fp8_gemm
fp8_gemm_blockwise_fwd:
  op: fp8_gemm_blockwise
  args: --op fp8_gemm_blockwise --baseline _cutlass --metrics tflops,speedup --only
    _triton,_cutlass
fp8_gemm_rowwise_fwd:
  op: fp8_gemm_rowwise
  args: --op fp8_gemm_rowwise --baseline _cutlass_or_ck --metrics tflops,speedup --only
    _triton,_cutlass_or_ck
fp8_gemm_rowwise_grouped_fwd:
  op: fp8_gemm_rowwise_grouped
  args: --op fp8_gemm_rowwise_grouped --baseline _cutlass_or_ck --metrics tflops,speedup
    --only _triton,_cutlass_or_ck
int4_gemm_fwd:
  op: int4_gemm
  args: --op int4_gemm --baseline tinygemm --metrics tflops,speedup --only triton,tinygemm
low_mem_dropout_fwd:
  op: low_mem_dropout
  args: --op low_mem_dropout --baseline torch_dropout --metrics tflops,speedup --only
    triton_dropout,torch_dropout
bf16_ragged_attention_fwd:
  op: ragged_attention
  args: --op ragged_attention --metrics tflops --only hstu_triton_ragged_attention
bf16_flash_attention_bwd:
  op: flash_attention
  args: --op flash_attention --baseline flash_v3 --metrics tflops,speedup --bwd --only
    triton_tutorial_flash_v2,triton_tutorial_flash_v2_opt,flash_v3
bf16_flex_attention_bwd:
  op: flex_attention
  args: --op flex_attention --baseline eager --metrics tflops,speedup --bwd --only
    compiled,eager
bf16_ragged_attention_bwd:
  op: ragged_attention
  args: --op ragged_attention --metrics tflops --bwd --only hstu_triton_ragged_attention
