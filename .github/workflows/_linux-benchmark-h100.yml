name: linux-benchmark-h100
on:
  workflow_call:
    secrets:
      TRITONBENCH_SCRIBE_GRAPHQL_ACCESS_TOKEN:
        required: True
        description: |
          Tritonbench Scribe Graph Access Token
    inputs:
      benchmark_name:
        required: True
        type: string
        description: |
          Benchmark name
      conda_env:
        required: True
        type: string
        description: |
          Conda environment to activate when testing Triton

jobs:
  linux-benchmark-h100:
    if: github.repository_owner == 'pytorch-labs'
    runs-on: [gcp-h100-runner]
    timeout-minutes: 240
    environment: docker-s3-upload
    env:
      SETUP_SCRIPT: "/workspace/setup_instance.sh"
      CONDA_ENV: ${{ inputs.conda_env }}
      JOB_NAME: tritonbench-h100-${{ inputs.conda_env }}-${{ inputs.benchmark_name }}
      TRITONBENCH_SCRIBE_GRAPHQL_ACCESS_TOKEN: ${{ secrets.TRITONBENCH_SCRIBE_GRAPHQL_ACCESS_TOKEN }}
    steps:
      - name: Checkout Tritonbench
        uses: actions/checkout@v3
        with:
          submodules: recursive
      - name: Tune Nvidia GPU
        run: |
          bash .ci/gpu/tune-gcp-h100.sh
          sudo ldconfig
          nvidia-smi
      - name: Benchmarking
        run: |
          bash .ci/tritonbench/run-benchmark.sh ${{ inputs.benchmark_name }}
          cp -r .benchmarks/${{ inputs.benchmark_name }} benchmark-output
      - name: Upload result to GH Actions Artifact
        uses: actions/upload-artifact@v4
        with:
          name: ${{ env.JOB_NAME }}
          path: benchmark-output/
      - name: Upload result to Scribe
        run: |
          . "${SETUP_SCRIPT}"
          latest_result_json=$(find ./benchmark-output/ -name "result.json"  | sort -r | head -n 1)
          python ./.ci/upload/scribe.py --json ${latest_result_json}
      - name: Restore Nvidia GPU
        if: always()
        run: |
          bash .ci/gpu/reset-gcp-h100.sh
          sudo ldconfig
          nvidia-smi
