# Tests we skip in triton-pytorch + OSS CI
# triton-pytorch is the triton version bundled with pytorch nightly
# We need to skip kernels that only work on triton-main
# Usage:
#  op-name: to skip an entire operator
#  op-name:\n\t- impl-name to skip an impl
flash_attention:
  # tma API changed in upstream
  - triton_tutorial_flash_v2_tma
  # triton_tutorial_*_ws kernels require triton-main
  - triton_tutorial_flash_v2_ws
  - triton_tutorial_flash_v2_tma_ws
  - triton_tutorial_flash_v2_tma_ws_persistent
# the two requires full fbgemm instead of genai flavor
fp8_gemm_rowwise:
fp8_gemm_rowwise_grouped:
fp8_gemm_grouped:
fp8_attention:
  # fp8 colfax_fmha is fbcode only
  - colfax_fmha
  # tma API changed in upstream
  - triton_flash_v2_tma
# fp8_fused_quant_gemm_rowwise requires fb-only kernels
fp8_fused_quant_gemm_rowwise:
gemm:
  # internal only kernels
  - hstu_triton_matmul
  # pt2 cutlass is broken
  - pt2_cutlass_matmul
# jagged tests are slow, so disable them in OSS
jagged_layer_norm:
jagged_mean:
jagged_softmax:
jagged_sum:
ragged_attention:
# cpu-op for testing
test_op:
# TODO: decoding attention requires updated xformers and flash_attn
# Which will RAM OOM on the CI machine
decoding_attention:
bwd_args:
  # flash_attention/triton_tutorial_flash_v2 does not support non-causal in backward
  flash_attention:
    - --causal
